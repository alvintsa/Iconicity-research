{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEjvwRfBSiOF"
   },
   "source": [
    "# Distributional assumption -- The distribution includes biases!\n",
    "\n",
    "Word vectors encode the linguistic contexts in which words appear. We can therefore ask how much linguistic bias -- which echoes societal biases -- is encoded in word vectors. Because much of web text is written by white, heterosexual North American men, as well as their Western European counterparts, the texts include biases that get encoded via distributional semantics methods.\n",
    "\n",
    "Societal biases include, but are not exclusive to\n",
    "\n",
    "* race\n",
    "* ethnicity\n",
    "* gender\n",
    "* sexuality\n",
    "* disabled status\n",
    "* age\n",
    "* intersections of all of these\n",
    "\n",
    "We know that neural language models produce text that amplifies these biases. For example, recent studies suggest that neural language models and chat systems like ChatGPT have a predominantly \"cool (white, affluent) mom\" orientation toward social problems. The models in production today have lots of bumpers to prevent producing racist, sexist, etc. text.\n",
    "\n",
    "For this, I will show you a cool projection method called \"subspace\" analysis, which produces a vector between two end-points.\n",
    "\n",
    "One of the first places studying gender bias in word vectors was done was in [Bolukbası et al. (2016)](https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf) who used a combination of:\n",
    "\n",
    "* Static word vectors from `word2vec`\n",
    "* Difference vectors between two \"endpoints\" of a binary spectrum (masculine <--> feminine)\n",
    "* Principal components analysis over that endpoint\n",
    "* The above steps produce a \"gender subspace\" vector\n",
    "* Compute similarity between each vector for an occupation -- which can be biased toward different genders to different degrees, and verified with census data -- and the gender subspace vector\n",
    "\n",
    "For our experiment, we will load in some vectors from `spaCy` and quantify the degree to which gender bias exists for some specific words:\n",
    "\n",
    "```python\n",
    "pairs = [\n",
    "  ('he', 'she'),\n",
    "  ('his', 'hers'),\n",
    "  ('him', 'her'),\n",
    "  ('his', 'her'),\n",
    "  ('John', 'Mary'),\n",
    "  ('himself', 'herself'),\n",
    "  ('father', 'mother'),\n",
    "  ('guy', 'gal'),\n",
    "  ('boy', 'girl'),\n",
    "  ('male', 'female')\n",
    "]\n",
    "```\n",
    "\n",
    "### Food for thought\n",
    "* Can you think of other pairs of words that might be useful here? (brother, sister), (wife, husband), (doctor, nurse) ?\n",
    "* What about other dimensions from one end to another might be able to use this method? age - young <--> old, first name last name\n",
    "* How can these \"endpoints\" be used for answering questions other than bias? maybe it can look into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTgj2WIslmcF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "! python -m spacy download en_core_web_md\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD # to project down into smaller dimension\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b84SCsMXVvX"
   },
   "source": [
    "## Get occupations\n",
    "\n",
    "There are lots of ways to get career or occupational bias. Here, we download a dataset that has lots of different biases -- gender, race, ethnicity -- and was used to study bias in GPT-2. We can check that the Bolukbası method correlates with:\n",
    "\n",
    "- The gender bias in US Census data\n",
    "- The gender bias that GPT-2 produces\n",
    "\n",
    "https://github.com/oxai/intersectional_gpt2\n",
    "\n",
    "Data:\n",
    "https://github.com/oxai/intersectional_gpt2/blob/master/data/GPT-2/US_data/us_rows_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "DRKauB4wXPa4",
    "outputId": "10362b3c-e8e3-4ec1-c47b-4b359a303b82"
   },
   "outputs": [],
   "source": [
    "# upload the file\n",
    "df = pd.read_csv(\"../documents/gpt_vs_us_data.csv\")\n",
    "df.head()\n",
    "\n",
    "# us_white_F is average bias in the census for white women\n",
    "# gpt_white_F is the average bias produced by GPT-2\n",
    "# ask dr jacobs what this means ~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_row(frame, field): # extract all data from a specific row in frame\n",
    "    result = []\n",
    "    for i, row in frame.iterrows():\n",
    "      row_val = row[field]\n",
    "      val_split = row_val.split(\"/\") # some data entries have multiple names separated by a \"/\"\n",
    "      for vals in val_split:\n",
    "        result.append(vals.strip()) # append to result array\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = extract_row(df, \"job\")\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MEp5izlXW8H"
   },
   "source": [
    "## Define the words used for the \"endpoints\" of the projection\n",
    "\n",
    "The idea here is that if we compute the \"difference\" (subtraction) between \"masculine\" vectors and \"feminine\" vectors, we can compute the subspace that corresponds to the axes that are correlated with some linguistic dimension of bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnsHaBftSeN6"
   },
   "outputs": [],
   "source": [
    "pairs = [\n",
    "  ('he', 'she'),\n",
    "  ('his', 'hers'),\n",
    "  ('him', 'her'),\n",
    "  ('his', 'her'),\n",
    "  ('John', 'Mary'),\n",
    "  ('himself', 'herself'),\n",
    "  ('father', 'mother'),\n",
    "  ('guy', 'gal'),\n",
    "  ('boy', 'girl'),\n",
    "  ('male', 'female'),\n",
    "  ('masculine', 'feminine'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoints(model, pairs): # get difference vectors between pairs, create endpoints\n",
    "    diff_vectors = [normalize(model(x).vector.reshape(1, -1) - model(y).vector.reshape(1, -1))[0] for x, y in pairs] #model(x) gets word embeddings from model\n",
    "    return diff_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_vectors = create_endpoints(nlp, pairs)\n",
    "diff_vectors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1Mm0TXtmKCN"
   },
   "source": [
    "# Learn the subspace using PCA\n",
    "\n",
    "PCA learns the dominant dimensions that characterize a space. If we project to a single dimension, this will result in one \"component\" that characterizes the differences between \"masculine\" vectors and \"feminine\" vectors.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/1920px-GaussianScatterPCA.svg.png\" width=400>\n",
    "\n",
    "PCA demonstration, Wikipeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_project(components, endpoints):\n",
    "    pca = TruncatedSVD(n_components=components) # one gender dimension\n",
    "    pca.fit(endpoints)\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "v8BihqbAdg2A",
    "outputId": "32105dfd-ba09-4d95-f86d-a1c3a06fa335"
   },
   "outputs": [],
   "source": [
    "pca = pca_project(1, diff_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64E1RbT3zF5V"
   },
   "source": [
    "As an annoying implementational detail, Bolukbası normalize word vectors to the unit circle -- but this might not be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_uc_vectors(model, vectors): # get unit circle vectors\n",
    "    uc_vectors = [normalize(model(vector).vector.reshape(1, -1))[0] for vector in vectors]\n",
    "    return uc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_vectors = compute_uc_vectors(nlp, jobs)\n",
    "job_vectors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hC7OJGcmNSz"
   },
   "source": [
    "# Compute the bias scores using cosine similarity\n",
    "\n",
    "We can look at how much the learned axis above \"weighs\" in the same direction as any particular job word vector. Above, we extract the vectors for a bunch of job titles, and we'll compute the cosine similarity (angle difference) between the \"gender subspace\" vector and the job title vector.\n",
    "\n",
    "The numbers weigh on the differences -- we subtracted feminine from masculine, so higher values indicate more feminine \"loadings\" and lower values are more \"masculine.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias(extracted, vectors): # get bias scores from df data, and created masculine/feminine subspace\n",
    "    bias_scores = []\n",
    "    for i, vector in enumerate(vectors):\n",
    "        vector_data = extracted[i]\n",
    "    bias_scores.append((vector_data, cosine(vector, normalize(pca.components_).reshape(-1)))) # make this its own function\n",
    "    return bias_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = pca.transform(job_vectors).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = list(zip(scores, jobs))\n",
    "scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "9AwHnbHMitx8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.205990</td>\n",
       "      <td>babysitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.184658</td>\n",
       "      <td>secretary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.164144</td>\n",
       "      <td>assistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004680</td>\n",
       "      <td>receptionist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.093552</td>\n",
       "      <td>cleaner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.233688</td>\n",
       "      <td>housekeeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.233688</td>\n",
       "      <td>maid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.213456</td>\n",
       "      <td>nurse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.039810</td>\n",
       "      <td>social worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.018482</td>\n",
       "      <td>teacher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.080389</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.065843</td>\n",
       "      <td>writer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.070280</td>\n",
       "      <td>barista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.276123</td>\n",
       "      <td>bartender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.013401</td>\n",
       "      <td>photographer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.073556</td>\n",
       "      <td>bus driver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.129740</td>\n",
       "      <td>reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.135650</td>\n",
       "      <td>journalist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.165756</td>\n",
       "      <td>cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.025915</td>\n",
       "      <td>doctor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.266651</td>\n",
       "      <td>manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.116236</td>\n",
       "      <td>janitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.186162</td>\n",
       "      <td>lawyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.152140</td>\n",
       "      <td>barber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.042828</td>\n",
       "      <td>chef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.233624</td>\n",
       "      <td>guard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.181002</td>\n",
       "      <td>security guard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.005182</td>\n",
       "      <td>bouncer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.157391</td>\n",
       "      <td>courier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.047276</td>\n",
       "      <td>computer programmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.221942</td>\n",
       "      <td>police officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.115445</td>\n",
       "      <td>taxi driver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.042086</td>\n",
       "      <td>chauffeur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.160397</td>\n",
       "      <td>driver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.130515</td>\n",
       "      <td>truck driver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.068778</td>\n",
       "      <td>construction worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.025428</td>\n",
       "      <td>laborer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.175933</td>\n",
       "      <td>carpenter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.094606</td>\n",
       "      <td>plumber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.206032</td>\n",
       "      <td>mechanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.057466</td>\n",
       "      <td>salesperson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bias                  job\n",
       "0  -0.205990           babysitter\n",
       "1   0.184658            secretary\n",
       "2   0.164144            assistant\n",
       "3   0.004680         receptionist\n",
       "4  -0.093552              cleaner\n",
       "5  -0.233688          housekeeper\n",
       "6  -0.233688                 maid\n",
       "7  -0.213456                nurse\n",
       "8  -0.039810        social worker\n",
       "9   0.018482              teacher\n",
       "10 -0.080389                model\n",
       "11  0.065843               writer\n",
       "12 -0.070280              barista\n",
       "13 -0.276123            bartender\n",
       "14  0.013401         photographer\n",
       "15  0.073556           bus driver\n",
       "16  0.129740             reporter\n",
       "17  0.135650           journalist\n",
       "18 -0.165756                 cook\n",
       "19  0.025915               doctor\n",
       "20  0.266651              manager\n",
       "21  0.116236              janitor\n",
       "22  0.186162               lawyer\n",
       "23  0.152140               barber\n",
       "24  0.042828                 chef\n",
       "25  0.233624                guard\n",
       "26  0.181002       security guard\n",
       "27 -0.005182              bouncer\n",
       "28  0.157391              courier\n",
       "29  0.047276  computer programmer\n",
       "30  0.221942       police officer\n",
       "31  0.115445          taxi driver\n",
       "32  0.042086            chauffeur\n",
       "33  0.160397               driver\n",
       "34  0.130515         truck driver\n",
       "35  0.068778  construction worker\n",
       "36 -0.025428              laborer\n",
       "37  0.175933            carpenter\n",
       "38  0.094606              plumber\n",
       "39  0.206032             mechanic\n",
       "40  0.057466          salesperson"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(scores_list, columns=['bias', 'job']) # create our own df from subsapce and jobs data\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDNB3oMh0uz1"
   },
   "source": [
    "### Correlation of spaCy vectors to GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot(df_one, df_two, x_axis, y_axis, label, x_axisName, y_axisName):\n",
    "    plot = (ggplot(df_one.merge(df_two),\n",
    "       aes(x=x_axis, y=y_axis, label=label)) + geom_label() + theme_bw()\n",
    "       + xlab(x_axisName)\n",
    "       + ylab(y_axisName))\n",
    "    return plot.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_bounded_plot(df_one, df_two, x_axis, y_axis, label, x_axisName, y_axisName, x_bounds):\n",
    "    plot = (ggplot(df_one.merge(df_two),\n",
    "       aes(x=x_axis, y=y_axis, label=label)) + geom_label() + theme_bw()\n",
    "       + xlab(x_axisName)\n",
    "       + ylab(y_axisName)\n",
    "       + xlim(x_bounds))\n",
    "    return plot.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.merge(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_plot(scores_df, df, \"bias\", \"gpt_white_F\", \"job\", \"spaCy vector bias, Bolukbasi method\", \"GPT vector bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2QPh2hP0yPX"
   },
   "source": [
    "### Correlation of spaCy vectors to US Census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_plot(scores_df, df, \"bias\", \"us_white_F\", \"job\", \"spaCy vector bias, Bolukbasi method\", \"US census bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Extracted dataset that studied GPT-2 bias\n",
    "2. Define endpoints for projection using spacy model. We used vectors that were pairs of opposite sides to each other. (masculine, feminine). We subtracted\n",
    "them. This produces difference vectors that we use as endpoints to project a subspace, which includes one \"component\" that characterizes the difference between\n",
    "masculine and feminine vectors. Now we have a subspace to utilize that has masculine and feminine endpoints, as well as one component that has the differences.\n",
    "3. We now compute the bias scores using cosine similarity - angle difference (the component). \n",
    "4. We compute the cosine similarity between the gender subspace and the job title vector.\n",
    "5. The numbers weigh on the differences -- we subtracted feminine from masculine, so higher values indicate more feminine \"loadings\" and lower values are more \"masculine.\"\n",
    "6. Using the bias score vectors, where it is (job title, bias score), we create a new dataframe to store these vectors.\n",
    "7. We then use this dataframe to create a correlation graph between the biases seen in the study and our bias scores.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "mBERT = BertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_japanese = [\n",
    "    ('彼', '彼女'),  # he, she or boyfriend, girlfriend\n",
    "    ('彼の', '彼女の'),  # his, her\n",
    "    ('男性', '女性'),  # male, female\n",
    "    ('男の子', '女の子'),  # boy, girl\n",
    "    ('父', '母'),  # father, mother\n",
    "]\n",
    "\n",
    "pairs_spanish =  [\n",
    "    ('él', 'ella'),  # he, she\n",
    "    ('su', 'su'),     # his, her (Note: Spanish has the same word for his and her)\n",
    "    ('hombre', 'mujer'),  # man, woman\n",
    "    ('padre', 'madre'),   # father, mother\n",
    "    ('niño', 'niña'),     # boy, girl\n",
    "    ('actor', 'actriz'),  # actor (male), actress (female)\n",
    "    ('rey', 'reina'),     # king, queen\n",
    "    ('hermano', 'hermana')\n",
    "]\n",
    "# brother, sister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embeddings from mBERT and create endpoints\n",
    "def mBERT_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = mBERT(**inputs)\n",
    "    # Take the mean of the embeddings across the token sequence to get a single embedding vector\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy().reshape(1,-1)\n",
    "    \n",
    "def mBERT_endpoints(vectors): # get difference vectors between pairs, create endpoints\n",
    "    diff_vectors = [normalize(mBERT_embeddings(x).reshape(1, -1) - mBERT_embeddings(y).reshape(1, -1))[0] for x, y in vectors] #model(x) gets word embeddings from model\n",
    "    return diff_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mBERT_vectors = mBERT_endpoints(pairs_japanese)\n",
    "mBERT_vectors # create endpoints for Japanese masculine<->feminine words to use for subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pca = pca_project(1, mBERT_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mBERT_vectors(model, vectors): # get unit circle vectors\n",
    "    uc_vectors = [normalize(mBERT_embeddings(word)).flatten() for word in vectors]\n",
    "    return uc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_vectors = compute_mBERT_vectors(mBERT, jobs)\n",
    "m_vectors # get number representations of jobs data with mBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This projects the job description vectors onto the gender subspace created by the PCA. \n",
    "# meaning each job vector is now represented by how much it aligns with the gender dimension derived from the Japanese word pairs\n",
    "mBERT_list = m_pca.transform(m_vectors).flatten() \n",
    "\n",
    "# pairs each projected value - the new value after pca transform \n",
    "# (indicating the job's alignment with the gender dimension) with its corresponding job description\n",
    "mBERT_zip = list(zip(mBERT_list, jobs))\n",
    "mBERT_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mBERT_df = pd.DataFrame(mBERT_zip, columns = [\"bias\", \"job\"]) # create jobs data frame\n",
    "mBERT_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher is more feminine, Lower is more masculine\n",
    "correlation_plot(mBERT_df, df, \"bias\", \"gpt_white_F\", \"job\", \"spaCy vector bias, Bolukbasi method\", \"GPT vector bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mBERT_spanish = mBERT_endpoints(pairs_spanish)\n",
    "mBERT_spanishPca = pca_project(1, mBERT_spanish)\n",
    "\n",
    "mBERT_spanish_jobs = compute_mBERT_vectors(mBERT, jobs)\n",
    "mBERT_spanish_list = list(zip(mBERT_spanishPca.transform(mBERT_spanish_jobs).flatten(), jobs))\n",
    "mBERT_spanish_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mBERT_spanish_df = pd.DataFrame(mBERT_spanish_list, columns = [\"bias\", \"job\"])\n",
    "mBERT_spanish_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher is more feminine, Lower is more masculine\n",
    "a = mBERT_spanish_df.merge(df)\n",
    "print(a)\n",
    "\n",
    "correlation_plot(mBERT_spanish_df, df, \"bias\", \"gpt_white_F\", \"job\", \"spaCy vector bias, Bolukbasi method\", \"GPT vector bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: JAPANESE JOBS\n",
    "japanese_jobs_df = pd.read_excel(\"../documents/iconicity_translations.xlsx\", sheet_name = \"Japanese\")\n",
    "# print(japanese_jobs_df.head())\n",
    "japanese_jobs = []\n",
    "for s in japanese_jobs_df.iloc[34:83, 2]:\n",
    "    if pd.notnull(s):\n",
    "        japanese_jobs.append(s)\n",
    "        \n",
    "# print(type(japanese_jobs[0]))\n",
    "\n",
    "mBERT_japaneseJobsPca = pca_project(1, mBERT_vectors)\n",
    "mBERT_japaneseJobs = compute_mBERT_vectors(mBERT, japanese_jobs)\n",
    "\n",
    "mBERT_japaneseJobs_list = list(zip(mBERT_japaneseJobsPca.transform(mBERT_japaneseJobs).flatten(), japanese_jobs))\n",
    "print(mBERT_japaneseJobs_list)\n",
    "mBERT_japanese_jobs_df = pd.DataFrame(mBERT_japaneseJobs_list, columns = [\"bias\", \"job\"])\n",
    "mBERT_japanese_jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mBERT_japanese_jobs_df[\"bias\"].min(), mBERT_japanese_jobs_df[\"bias\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Path to the Noto Sans CJK JP font file on your computer\n",
    "font_path = \"../Dancing_Script,Nanum_Brush_Script,Noto_Sans_JP,Rubik_Iso,Style_Script,etc/Noto_Sans_JP/NotoSansJP-VariableFont_wght.ttf\"\n",
    "\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "fm.fontManager.addfont(font_path)\n",
    "\n",
    "if font_name in [f.name for f in fm.fontManager.ttflist]:\n",
    "    print(f\"Font '{font_name}' successfully registered with Matplotlib.\")\n",
    "else:\n",
    "    print(f\"Failed to register font '{font_name}' with Matplotlib.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "\n",
    "job_title_mapping = {\n",
    "    \"女優\": \"actress\",\n",
    "    \"アーティスト\": \"artist\",\n",
    "    \"弁護士\": \"attorney\",\n",
    "    \"ベビーシッター\": \"babysitter\",\n",
    "    \"ボス\": \"boss\",\n",
    "    \"ビジネスマン\": \"businessman\",\n",
    "    \"実業家\": \"businesswoman\",\n",
    "    \"大工\": \"carpenter\",\n",
    "    \"シェフ\": \"chef\",\n",
    "    \"コーチ\": \"coach\",\n",
    "    \"警官\": \"police officer\",\n",
    "    \"踊り子\": \"dancer\",\n",
    "    \"探偵\": \"detective\",\n",
    "    \"医者\": \"doctor\",\n",
    "    \"運転者\": \"driver\",\n",
    "    \"消防士\": \"firefighter\",\n",
    "    \"庭師\": \"gardener\",\n",
    "    \"ガード\": \"guard / security guard / bouncer\",\n",
    "    \"お手伝いさん\": \"cleaner / housekeeper / maid\",\n",
    "    \"管理人\": \"janitor\",\n",
    "    \"裁判官\": \"judge\",\n",
    "    \"王\": \"king\",\n",
    "    \"弁護士\": \"lawyer\",\n",
    "    \"図書館員\": \"librarian\",\n",
    "    \"中尉\": \"lieutenant\",\n",
    "    \"メイド\": \"maid\",\n",
    "    \"郵便配達員\": \"mailman\",\n",
    "    \"マネージャー\": \"manager\",\n",
    "    \"メカニック\": \"mechanic\",\n",
    "    \"看護師\": \"nurse\",\n",
    "    \"パイロット\": \"pilot\",\n",
    "    \"社長\": \"president\",\n",
    "    \"王子\": \"prince\",\n",
    "    \"お姫様\": \"princess\",\n",
    "    \"囚人\": \"prisoner\",\n",
    "    \"教授\": \"professor\",\n",
    "    \"精神科医\": \"psychiatrist\",\n",
    "    \"女王\": \"queen\",\n",
    "    \"秘書\": \"secretary / assistant\",\n",
    "    \"シェリフ\": \"sheriff\",\n",
    "    \"兵士\": \"soldier\",\n",
    "    \"学生\": \"student\",\n",
    "    \"教師\": \"teacher\",\n",
    "    \"泥棒\": \"thief\",\n",
    "    \"ウェイター\": \"waiter\",\n",
    "    \"ウェイトレス\": \"waitress\",\n",
    "    \"ライター\": \"writer\"\n",
    "}\n",
    "\n",
    "english_to_japanese = {\n",
    "   \"actress\": \"女優\",\n",
    "    \"artist\": \"アーティスト\",\n",
    "    \"attorney\": \"弁護士\",\n",
    "    \"babysitter\": \"ベビーシッター\",\n",
    "    \"boss\": \"ボス\",\n",
    "    \"businessman\": \"ビジネスマン\",\n",
    "    \"businesswoman\": \"実業家\",\n",
    "    \"carpenter\": \"大工\",\n",
    "    \"chef\": \"シェフ\",\n",
    "    \"coach\": \"コーチ\",\n",
    "    \"police officer\": \"警官\",\n",
    "    \"dancer\": \"踊り子\",\n",
    "    \"detective\": \"探偵\",\n",
    "    \"doctor\": \"医者\",\n",
    "    \"driver\": \"運転者\",\n",
    "    \"firefighter\": \"消防士\",\n",
    "    \"gardener\": \"庭師\",\n",
    "    \"guard / security guard / bouncer\": \"ガード\",\n",
    "    \"cleaner / housekeeper / maid\": \"お手伝いさん\",\n",
    "    \"janitor\": \"管理人\",\n",
    "    \"judge\": \"裁判官\",\n",
    "    \"king\": \"王\",\n",
    "    \"lawyer\": \"弁護士\",\n",
    "    \"librarian\": \"図書館員\",\n",
    "    \"lieutenant\": \"中尉\",\n",
    "    \"maid\": \"メイド\",\n",
    "    \"mailman\": \"郵便配達員\",\n",
    "    \"manager\": \"マネージャー\",\n",
    "    \"mechanic\": \"メカニック\",\n",
    "    \"nurse\": \"看護師\",\n",
    "    \"pilot\": \"パイロット\",\n",
    "    \"president\": \"社長\",\n",
    "    \"prince\": \"王子\",\n",
    "    \"princess\": \"お姫様\",\n",
    "    \"prisoner\": \"囚人\",\n",
    "    \"professor\": \"教授\",\n",
    "    \"psychiatrist\": \"精神科医\",\n",
    "    \"queen\": \"女王\",\n",
    "    \"secretary / assistant\": \"秘書\",\n",
    "    \"sheriff\": \"シェリフ\",\n",
    "    \"soldier\": \"兵士\",\n",
    "    \"student\": \"学生\",\n",
    "    \"teacher\": \"教師\",\n",
    "    \"thief\": \"泥棒\",\n",
    "    \"waiter\": \"ウェイター\",\n",
    "    \"waitress\": \"ウェイトレス\",\n",
    "    \"writer\": \"ライター\"\n",
    "}\n",
    "\n",
    "# TODO: check mapping values with other data\n",
    "# TODO: make df to csv\n",
    "# print(mBERT_japanese_jobs_df)\n",
    "\n",
    "japanese_to_english_jobs_list = []\n",
    "\n",
    "for s in mBERT_japaneseJobs_list:\n",
    "    pair = list(s)\n",
    "    if pair[1] in job_title_mapping:\n",
    "        pair[1] = job_title_mapping[pair[1]]\n",
    "        japanese_to_english_jobs_list.append(tuple(pair))\n",
    "        \n",
    "    \n",
    "# print(japanese_to_english_jobs_list)\n",
    "    \n",
    "mBERT_japanese_jobs_df = pd.DataFrame(japanese_to_english_jobs_list, columns = [\"bias\", \"job\"])\n",
    "\n",
    "merged_df = mBERT_japanese_jobs_df.merge(df)\n",
    "\n",
    "display(df)\n",
    "display(mBERT_japanese_jobs_df)\n",
    "display(merged_df)\n",
    "# print(mBERT_japanese_jobs_df)\n",
    "\n",
    "correlation_bounded_plot(mBERT_japanese_jobs_df, df, \"bias\", \"gpt_white_F\", \"job\", \"Japanese Job Bias\", \"GPT vector bias\", (-0.7, 0.7))\n",
    "# (ggplot(merged_df,\n",
    "#        aes(x='bias', y='gpt_white_F', label='job')) + geom_label(size = 5, label_padding=0.1) + theme_bw()\n",
    "#        + xlab(\"Japanese bias\")\n",
    "#        + ylab(\"GPT vector bias\")\n",
    "#     #    + xlim(-0.7, 0.7)\n",
    "#     #    + ylim(0.7, 1.4)  \n",
    "#        + theme(text=element_text(family=prop.get_name())))\n",
    "\n",
    "\n",
    "# a = mBERT_japanese_jobs_df.merge(df)\n",
    "# print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(scores_df.head())\n",
    "display(scores_df.merge(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Register the font with Matplotlib\n",
    "# print(mBERT_japanese_jobs_df)\n",
    "# print(df)\n",
    "\n",
    "# merged_df = pd.merge(mBERT_japanese_jobs_df, df, how='cross')\n",
    "# # merged_df = mBERT_japanese_jobs_df.merge(df)\n",
    "# # print(merged_df)\n",
    "# # print(merged_df)\n",
    "\n",
    "# # correlation_plot(mBERT_japanese_jobs_df, df, \"bias\", \"gpt_white_F\", \"job\", \"spaCy vector bias, Bolukbasi method\", \"GPT vector bias\")\n",
    "# (ggplot(merged_df,\n",
    "#        aes(x='bias', y='gpt_white_F', label='job_x')) + geom_label(size = 5, label_padding=0.1) + theme_bw()\n",
    "#        + xlab(\"Japanese bias\")\n",
    "#        + ylab(\"GPT vector bias\")\n",
    "#     #    + xlim(-0.7, 0.7)\n",
    "#     #    + ylim(0.7, 1.4)  \n",
    "#        + theme(text=element_text(family=prop.get_name())))\n",
    "\n",
    "\n",
    "# # a = mBERT_japanese_jobs_df.merge(df)\n",
    "# # print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "22bc9f29c608090ef5c32fffe2f8088bbf12d521771a2adf38594eceebc29062"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
